{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6720 images belonging to 6 classes.\n",
      "Found 2400 images belonging to 6 classes.\n",
      "Train on 6720 samples, validate on 2400 samples\n",
      "Epoch 1/50\n",
      "6720/6720 [==============================] - 31s 5ms/step - loss: 1.4023 - acc: 0.7060 - val_loss: 0.6040 - val_acc: 0.7987\n",
      "Epoch 2/50\n",
      "6720/6720 [==============================] - 31s 5ms/step - loss: 0.5846 - acc: 0.8060 - val_loss: 0.6069 - val_acc: 0.8242\n",
      "Epoch 3/50\n",
      "6720/6720 [==============================] - 31s 5ms/step - loss: 0.5022 - acc: 0.8384 - val_loss: 0.5849 - val_acc: 0.8408\n",
      "Epoch 4/50\n",
      "6720/6720 [==============================] - 31s 5ms/step - loss: 0.4767 - acc: 0.8496 - val_loss: 0.5963 - val_acc: 0.8346\n",
      "Epoch 5/50\n",
      "6720/6720 [==============================] - 31s 5ms/step - loss: 0.4157 - acc: 0.8674 - val_loss: 0.9266 - val_acc: 0.8146\n",
      "Epoch 6/50\n",
      "6720/6720 [==============================] - 31s 5ms/step - loss: 0.4069 - acc: 0.8738 - val_loss: 0.8594 - val_acc: 0.8492\n",
      "Epoch 7/50\n",
      "6720/6720 [==============================] - 31s 5ms/step - loss: 0.3844 - acc: 0.8737 - val_loss: 0.8222 - val_acc: 0.8175\n",
      "Epoch 8/50\n",
      "6720/6720 [==============================] - 31s 5ms/step - loss: 0.3641 - acc: 0.8909 - val_loss: 0.8443 - val_acc: 0.8408\n",
      "Epoch 9/50\n",
      "6720/6720 [==============================] - 31s 5ms/step - loss: 0.3669 - acc: 0.8948 - val_loss: 0.7033 - val_acc: 0.8458\n",
      "Epoch 10/50\n",
      "6720/6720 [==============================] - 31s 5ms/step - loss: 0.3268 - acc: 0.9051 - val_loss: 0.9791 - val_acc: 0.8304\n",
      "Epoch 11/50\n",
      "6720/6720 [==============================] - 31s 5ms/step - loss: 0.3136 - acc: 0.9039 - val_loss: 0.7959 - val_acc: 0.8462\n",
      "Epoch 12/50\n",
      "6720/6720 [==============================] - 31s 5ms/step - loss: 0.3184 - acc: 0.9122 - val_loss: 0.9106 - val_acc: 0.8504\n",
      "Epoch 13/50\n",
      "6720/6720 [==============================] - 31s 5ms/step - loss: 0.3047 - acc: 0.9112 - val_loss: 1.2617 - val_acc: 0.8129\n",
      "Epoch 14/50\n",
      "6720/6720 [==============================] - 31s 5ms/step - loss: 0.3051 - acc: 0.9153 - val_loss: 0.9345 - val_acc: 0.8504\n",
      "Epoch 15/50\n",
      "6720/6720 [==============================] - 31s 5ms/step - loss: 0.2879 - acc: 0.9223 - val_loss: 0.8818 - val_acc: 0.8558\n",
      "Epoch 16/50\n",
      "6720/6720 [==============================] - 31s 5ms/step - loss: 0.3043 - acc: 0.9155 - val_loss: 1.1358 - val_acc: 0.8425\n",
      "Epoch 17/50\n",
      "6720/6720 [==============================] - 31s 5ms/step - loss: 0.2809 - acc: 0.9274 - val_loss: 1.0658 - val_acc: 0.8458\n",
      "Epoch 18/50\n",
      "6720/6720 [==============================] - 31s 5ms/step - loss: 0.2834 - acc: 0.9253 - val_loss: 1.0443 - val_acc: 0.8517\n",
      "Epoch 19/50\n",
      "6720/6720 [==============================] - 31s 5ms/step - loss: 0.2857 - acc: 0.9274 - val_loss: 1.1528 - val_acc: 0.8517\n",
      "Epoch 20/50\n",
      "6720/6720 [==============================] - 31s 5ms/step - loss: 0.2589 - acc: 0.9286 - val_loss: 1.4856 - val_acc: 0.8292\n",
      "Epoch 21/50\n",
      "6720/6720 [==============================] - 31s 5ms/step - loss: 0.2569 - acc: 0.9335 - val_loss: 1.0364 - val_acc: 0.8662\n",
      "Epoch 22/50\n",
      "6720/6720 [==============================] - 32s 5ms/step - loss: 0.2519 - acc: 0.9378 - val_loss: 1.1649 - val_acc: 0.8517\n",
      "Epoch 23/50\n",
      "6720/6720 [==============================] - 31s 5ms/step - loss: 0.2607 - acc: 0.9385 - val_loss: 1.0847 - val_acc: 0.8575\n",
      "Epoch 24/50\n",
      "6720/6720 [==============================] - 32s 5ms/step - loss: 0.2315 - acc: 0.9391 - val_loss: 1.5229 - val_acc: 0.8346\n",
      "Epoch 25/50\n",
      "6720/6720 [==============================] - 31s 5ms/step - loss: 0.2867 - acc: 0.9376 - val_loss: 1.2439 - val_acc: 0.8521\n",
      "Epoch 26/50\n",
      "6720/6720 [==============================] - 32s 5ms/step - loss: 0.2414 - acc: 0.9391 - val_loss: 1.2098 - val_acc: 0.8533\n",
      "Epoch 27/50\n",
      "6720/6720 [==============================] - 31s 5ms/step - loss: 0.2678 - acc: 0.9393 - val_loss: 1.4363 - val_acc: 0.8392\n",
      "Epoch 28/50\n",
      "6720/6720 [==============================] - 32s 5ms/step - loss: 0.2322 - acc: 0.9424 - val_loss: 1.3209 - val_acc: 0.8500\n",
      "Epoch 29/50\n",
      "6720/6720 [==============================] - 32s 5ms/step - loss: 0.2392 - acc: 0.9458 - val_loss: 1.3025 - val_acc: 0.8429\n",
      "Epoch 30/50\n",
      "6720/6720 [==============================] - 32s 5ms/step - loss: 0.2615 - acc: 0.9440 - val_loss: 1.2795 - val_acc: 0.8571\n",
      "Epoch 31/50\n",
      "6720/6720 [==============================] - 32s 5ms/step - loss: 0.2539 - acc: 0.9442 - val_loss: 1.2582 - val_acc: 0.8633\n",
      "Epoch 32/50\n",
      "6720/6720 [==============================] - 32s 5ms/step - loss: 0.2294 - acc: 0.9497 - val_loss: 1.2534 - val_acc: 0.8608\n",
      "Epoch 33/50\n",
      "6720/6720 [==============================] - 32s 5ms/step - loss: 0.2377 - acc: 0.9479 - val_loss: 1.6049 - val_acc: 0.8204\n",
      "Epoch 34/50\n",
      "6720/6720 [==============================] - 32s 5ms/step - loss: 0.2380 - acc: 0.9479 - val_loss: 1.3551 - val_acc: 0.8479\n",
      "Epoch 35/50\n",
      "6720/6720 [==============================] - 32s 5ms/step - loss: 0.2391 - acc: 0.9464 - val_loss: 1.3913 - val_acc: 0.8483\n",
      "Epoch 36/50\n",
      "6720/6720 [==============================] - 32s 5ms/step - loss: 0.2211 - acc: 0.9528 - val_loss: 1.3596 - val_acc: 0.8558\n",
      "Epoch 37/50\n",
      "6720/6720 [==============================] - 32s 5ms/step - loss: 0.2279 - acc: 0.9516 - val_loss: 1.5062 - val_acc: 0.8462\n",
      "Epoch 38/50\n",
      "6720/6720 [==============================] - 32s 5ms/step - loss: 0.2328 - acc: 0.9521 - val_loss: 1.3074 - val_acc: 0.8517\n",
      "Epoch 39/50\n",
      "6720/6720 [==============================] - 32s 5ms/step - loss: 0.2229 - acc: 0.9478 - val_loss: 1.5316 - val_acc: 0.8442\n",
      "Epoch 40/50\n",
      "6720/6720 [==============================] - 32s 5ms/step - loss: 0.2337 - acc: 0.9518 - val_loss: 1.3602 - val_acc: 0.8538\n",
      "Epoch 41/50\n",
      "6720/6720 [==============================] - 32s 5ms/step - loss: 0.1872 - acc: 0.9586 - val_loss: 1.7344 - val_acc: 0.8308\n",
      "Epoch 42/50\n",
      "6720/6720 [==============================] - 32s 5ms/step - loss: 0.2363 - acc: 0.9548 - val_loss: 1.3275 - val_acc: 0.8508\n",
      "Epoch 43/50\n",
      "6720/6720 [==============================] - 32s 5ms/step - loss: 0.2399 - acc: 0.9504 - val_loss: 1.6275 - val_acc: 0.8358\n",
      "Epoch 44/50\n",
      "6720/6720 [==============================] - 32s 5ms/step - loss: 0.2250 - acc: 0.9487 - val_loss: 1.5409 - val_acc: 0.8379\n",
      "Epoch 45/50\n",
      "6720/6720 [==============================] - 32s 5ms/step - loss: 0.2177 - acc: 0.9564 - val_loss: 1.3325 - val_acc: 0.8521\n",
      "Epoch 46/50\n",
      "6720/6720 [==============================] - 32s 5ms/step - loss: 0.2041 - acc: 0.9537 - val_loss: 1.4633 - val_acc: 0.8525\n",
      "Epoch 47/50\n",
      "6720/6720 [==============================] - 32s 5ms/step - loss: 0.2232 - acc: 0.9510 - val_loss: 1.8654 - val_acc: 0.8246\n",
      "Epoch 48/50\n",
      "6720/6720 [==============================] - 32s 5ms/step - loss: 0.2128 - acc: 0.9530 - val_loss: 1.4205 - val_acc: 0.8546\n",
      "Epoch 49/50\n",
      "6720/6720 [==============================] - 32s 5ms/step - loss: 0.1985 - acc: 0.9610 - val_loss: 1.4752 - val_acc: 0.8612\n",
      "Epoch 50/50\n",
      "6720/6720 [==============================] - 32s 5ms/step - loss: 0.2113 - acc: 0.9574 - val_loss: 1.5186 - val_acc: 0.8512\n"
     ]
    }
   ],
   "source": [
    "# This script goes along the blog post\n",
    "# \"Building powerful image classification models using very little data\"\n",
    "# from blog.keras.io.\n",
    "# Modifications by Yannis Georgas 5-Jan-2019\n",
    "# data for training and validation sets was scrapped and cleaned from instagram, flickr and other websites\n",
    "\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras import applications\n",
    "\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 250, 250\n",
    "\n",
    "top_model_weights_path = 'bottleneck_fc_model_smax.h5'\n",
    "train_data_dir = 'data/Train'\n",
    "validation_data_dir = 'data/Validation'\n",
    "nb_train_samples = 3360  # that is 3 times 1120 \n",
    "nb_validation_samples = 1200   # 1200/ 3 = 400\n",
    "epochs = 50\n",
    "batch_size = 16 # 3360 / 16 = 210 (clean integer) and for validation 1200/16 = 75 (clean integer) \n",
    "#if its not clean integer it will blow up because its not able to divide for mini batch calculation\n",
    "\n",
    "\n",
    "def save_bottlebeck_features():\n",
    "    datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "    # build the VGG16 network\n",
    "    model = applications.VGG16(include_top=False, weights='imagenet')\n",
    "\n",
    "    generator = datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "    bottleneck_features_train = model.predict_generator(\n",
    "        generator, nb_train_samples // batch_size)\n",
    "    np.save('bottleneck_features_train_smax.npy',bottleneck_features_train)\n",
    "\n",
    "    generator = datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "    bottleneck_features_validation = model.predict_generator(\n",
    "        generator, nb_validation_samples // batch_size)\n",
    "    np.save('bottleneck_features_validation_smax.npy',bottleneck_features_validation)\n",
    "\n",
    "\n",
    "def train_top_model():\n",
    "        \n",
    "    k = 3 # number of classes but actually \n",
    "    \n",
    "    train_data = np.load('bottleneck_features_train_smax.npy')\n",
    "    train_labels = np.array([0] * (nb_train_samples // k) + \n",
    "                            [1] * (nb_train_samples // k) + \n",
    "                            [2] * (nb_train_samples // k)) \n",
    "\n",
    "    validation_data = np.load('bottleneck_features_validation_smax.npy')\n",
    "    validation_labels = np.array([0] * (nb_validation_samples // k) + \n",
    "                                 [1] * (nb_validation_samples // k) + \n",
    "                                 [2] * (nb_validation_samples // k))\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(k, activation='softmax'))  # 3 for tshirt, hoodie, blouse - softmax (Dense 1 for binary class - sigmoid)\n",
    "\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='sparse_categorical_crossentropy', metrics=['accuracy'])   #binary_crossentropy for sigmoid\n",
    "\n",
    "    model.fit(train_data, train_labels,\n",
    "              epochs=epochs,\n",
    "              batch_size=batch_size,\n",
    "              validation_data=(validation_data, validation_labels))\n",
    "    model.save_weights(top_model_weights_path)\n",
    "    \n",
    "    \n",
    "save_bottlebeck_features()\n",
    "train_top_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE    \n",
    "    If your targets are one-hot encoded, use categorical_crossentropy.\n",
    "        Examples of one-hot encodings:\n",
    "            [1,0,0]\n",
    "            [0,1,0]\n",
    "            [0,0,1]\n",
    "    But if your targets are integers, use sparse_categorical_crossentropy.\n",
    "        Examples of integer encodings (for the sake of completion):\n",
    "            1\n",
    "            2\n",
    "            3\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
